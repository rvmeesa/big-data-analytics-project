{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Rameesa Nadeem##\n",
        "##Fa21-Bds-024##"
      ],
      "metadata": {
        "id": "o6BM3qd1nD6i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BIG DATA ANALYSIS PROJECT#\n",
        "*On Textual data scrapped and ingested from website hubspot*"
      ],
      "metadata": {
        "id": "G2G03XCymklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installing Pyspark to run it on colab because gradio doesnot work on databricks#\n"
      ],
      "metadata": {
        "id": "7xQOulatalid"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bbk4hLMmH-Yw",
        "outputId": "9eec46fa-0325-401b-c37e-1921164e9a53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.11/dist-packages (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install pyspark\n",
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Enusuring latest version is installed*"
      ],
      "metadata": {
        "id": "fRevSa_Ca5ct"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vdjoXLIIdB-",
        "outputId": "1c923531-e3ab-4b9f-a456-f36f40a63b1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: pyspark\n",
            "Version: 3.5.4\n",
            "Summary: Apache Spark Python API\n",
            "Home-page: https://github.com/apache/spark/tree/master/python\n",
            "Author: Spark Developers\n",
            "Author-email: dev@spark.apache.org\n",
            "License: http://www.apache.org/licenses/LICENSE-2.0\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: py4j\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Setting Java Path*"
      ],
      "metadata": {
        "id": "rs2eO9sEbACk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCMWMGXWIGwi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import findspark\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/usr/local/lib/python3.11/dist-packages/pyspark\"  # Update this path\n",
        "\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Creating Spark Session*"
      ],
      "metadata": {
        "id": "H_QbFep9bENV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "m4FrIoALIiKd",
        "outputId": "d3f02eff-1815-483f-a38d-66860526743d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f11741522d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://58a27708f3e5:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.4</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>ColabPySpark</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ColabPySpark\").getOrCreate()\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Scrapping and Ingesting the data from the hubspot website##"
      ],
      "metadata": {
        "id": "FZkt0jl1eSxF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5DAW-CXIrpT",
        "outputId": "878faea8-b7be-48bf-8c51-f125089f92a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped data saved as 'marketing_data.csv'\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "url = \"https://blog.hubspot.com/marketing\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "# Extract relevant content (e.g., headlines)\n",
        "data = {\"content\": [item.get_text(strip=True) for item in soup.find_all(\"h2\")]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save the data as a CSV locally\n",
        "df.to_csv(\"marketing_data.csv\", index=False)\n",
        "print(\"Scraped data saved as 'marketing_data.csv'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Few Columns of csv looks like this*"
      ],
      "metadata": {
        "id": "SM9Omqhfec__"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Al_bFuAHI5FX",
        "outputId": "bdc54504-b984-43f9-c222-fa09bad988db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------------------+\n",
            "|content                                                        |\n",
            "+---------------------------------------------------------------+\n",
            "|6 Steps to Create an Outstanding Marketing Plan [Free Templa...|\n",
            "|Featured Articles                                              |\n",
            "|Latest articles                                                |\n",
            "|Artificial Intelligence                                        |\n",
            "|Instagram Marketing                                            |\n",
            "|From HubSpot's video library                                   |\n",
            "|Marketing Strategy                                             |\n",
            "|From the HubSpot Podcast Network                               |\n",
            "|More content                                                   |\n",
            "|Visit the HubSpot blogs                                        |\n",
            "|Subscribe to HubSpot's Newsletters                             |\n",
            "|Join 600,000+ Fellow MarketersThanks for Subscribing!          |\n",
            "|Popular Features                                               |\n",
            "|Free Tools                                                     |\n",
            "|Company                                                        |\n",
            "|Customers                                                      |\n",
            "|Partners                                                       |\n",
            "+---------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data_path = \"marketing_data.csv\"\n",
        "data = spark.read.csv(data_path, header=True, inferSchema=True)\n",
        "\n",
        "data.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Preprocessing##"
      ],
      "metadata": {
        "id": "5OZKRqnbeoBJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Removing Punctuation converting to lower case and tokenizing the text*"
      ],
      "metadata": {
        "id": "wkx5EikOetKe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBP28vckI_c_",
        "outputId": "4511f190-5884-4a70-8655-213830a2e630"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------------------------+---------------------------------------------------------------------+\n",
            "|content                                                   |tokens                                                               |\n",
            "+----------------------------------------------------------+---------------------------------------------------------------------+\n",
            "| steps to create an outstanding marketing plan free templa|[, steps, to, create, an, outstanding, marketing, plan, free, templa]|\n",
            "|featured articles                                         |[featured, articles]                                                 |\n",
            "|latest articles                                           |[latest, articles]                                                   |\n",
            "|artificial intelligence                                   |[artificial, intelligence]                                           |\n",
            "|instagram marketing                                       |[instagram, marketing]                                               |\n",
            "|from hubspots video library                               |[from, hubspots, video, library]                                     |\n",
            "|marketing strategy                                        |[marketing, strategy]                                                |\n",
            "|from the hubspot podcast network                          |[from, the, hubspot, podcast, network]                               |\n",
            "|more content                                              |[more, content]                                                      |\n",
            "|visit the hubspot blogs                                   |[visit, the, hubspot, blogs]                                         |\n",
            "|subscribe to hubspots newsletters                         |[subscribe, to, hubspots, newsletters]                               |\n",
            "|join  fellow marketersthanks for subscribing              |[join, fellow, marketersthanks, for, subscribing]                    |\n",
            "|popular features                                          |[popular, features]                                                  |\n",
            "|free tools                                                |[free, tools]                                                        |\n",
            "|company                                                   |[company]                                                            |\n",
            "|customers                                                 |[customers]                                                          |\n",
            "|partners                                                  |[partners]                                                           |\n",
            "+----------------------------------------------------------+---------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, lower, regexp_replace, split\n",
        "cleaned_data = (\n",
        "    data.withColumn(\"content\", lower(col(\"content\")))\n",
        "         .withColumn(\"content\", regexp_replace(col(\"content\"), \"[^a-zA-Z\\s]\", \"\"))\n",
        "         .withColumn(\"tokens\", split(col(\"content\"), r\"\\s+\"))\n",
        ")\n",
        "\n",
        "cleaned_data.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Counting Words frequency*"
      ],
      "metadata": {
        "id": "CsvFna36e4-d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUGoFqbDJBmN",
        "outputId": "f45f88f1-0c45-4410-daf1-42140332c56d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-----+\n",
            "|           word|count|\n",
            "+---------------+-----+\n",
            "|      marketing|    3|\n",
            "|       articles|    2|\n",
            "|           free|    2|\n",
            "|       hubspots|    2|\n",
            "|            the|    2|\n",
            "|           from|    2|\n",
            "|        hubspot|    2|\n",
            "|             to|    2|\n",
            "|      instagram|    1|\n",
            "|        popular|    1|\n",
            "|           more|    1|\n",
            "|    subscribing|    1|\n",
            "|            for|    1|\n",
            "|    outstanding|    1|\n",
            "|        library|    1|\n",
            "|        content|    1|\n",
            "|       featured|    1|\n",
            "|marketersthanks|    1|\n",
            "|         create|    1|\n",
            "|       partners|    1|\n",
            "+---------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import explode\n",
        "word_freq = (\n",
        "    cleaned_data.withColumn(\"word\", explode(col(\"tokens\")))\n",
        "                .groupBy(\"word\")\n",
        "                .count()\n",
        "                .orderBy(\"count\", ascending=False)\n",
        ")\n",
        "\n",
        "word_freq.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Statistical Analysis#"
      ],
      "metadata": {
        "id": "dCaII2rBh7UI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Computing Variance and Correlation*"
      ],
      "metadata": {
        "id": "7LlBKJcTh-_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_statistics(cleaned_data):\n",
        "    # Compute variance of word frequencies\n",
        "    word_freq = (\n",
        "        cleaned_data.withColumn(\"word\", explode(col(\"tokens\")))\n",
        "                    .groupBy(\"word\")\n",
        "                    .count()\n",
        "    )\n",
        "    word_freq_variance = word_freq.selectExpr(\"VAR_SAMP(count) as variance\").collect()[0]['variance']\n",
        "\n",
        "    # Compute correlation between content length and number of words\n",
        "    cleaned_data = cleaned_data.withColumn(\"word_count\", length(col(\"tokens\")))\n",
        "    correlation = cleaned_data.stat.corr(\"content_length\", \"word_count\")\n",
        "\n",
        "    return word_freq_variance, correlation\n",
        "\n",
        "# Define the ingestion, processing, and analysis workflow\n",
        "def data_workflow():\n",
        "    df = ingest_data()\n",
        "    cleaned_data, word_freq = process_data()\n",
        "\n",
        "    word_freq_variance, correlation = compute_statistics(cleaned_data)\n",
        "    return (\n",
        "        f\"Data Ingested Successfully: {len(df)} rows\",\n",
        "        cleaned_data.toPandas(),\n",
        "        word_freq,\n",
        "        f\"Word Frequency Variance: {word_freq_variance}\",\n",
        "        f\"Content Length vs Word Count Correlation: {correlation}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "3662DAg1KkDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Machine Learning Algorithms#"
      ],
      "metadata": {
        "id": "Gj5R6u07iKh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*K Means Algorithm*"
      ],
      "metadata": {
        "id": "XpGFIHnOi-0d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUvMXbbFJFNM",
        "outputId": "9d3c9c1b-78eb-4f7a-eb50-1a21c580c559"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "|             content|              tokens|        raw_features|            features|prediction|\n",
            "+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "| steps to create ...|[, steps, to, cre...|(100,[4,23,26,42,...|(100,[4,23,26,42,...|         4|\n",
            "|   featured articles|[featured, articles]|(100,[62,71],[1.0...|(100,[62,71],[2.1...|         2|\n",
            "|     latest articles|  [latest, articles]|(100,[44,71],[1.0...|(100,[44,71],[1.7...|         2|\n",
            "|artificial intell...|[artificial, inte...|(100,[32,54],[1.0...|(100,[32,54],[2.1...|         0|\n",
            "| instagram marketing|[instagram, marke...|(100,[42,47],[1.0...|(100,[42,47],[1.2...|         0|\n",
            "|from hubspots vid...|[from, hubspots, ...|(100,[3,4,21,74],...|(100,[3,4,21,74],...|         1|\n",
            "|  marketing strategy|[marketing, strat...|(100,[42,91],[1.0...|(100,[42,91],[1.2...|         0|\n",
            "|from the hubspot ...|[from, the, hubsp...|(100,[17,21,22,52...|(100,[17,21,22,52...|         0|\n",
            "|        more content|     [more, content]|(100,[29,38],[1.0...|(100,[29,38],[2.1...|         0|\n",
            "|visit the hubspot...|[visit, the, hubs...|(100,[17,22,88,98...|(100,[17,22,88,98...|         0|\n",
            "|subscribe to hubs...|[subscribe, to, h...|(100,[4,22,50,88]...|(100,[4,22,50,88]...|         0|\n",
            "|join  fellow mark...|[join, fellow, ma...|(100,[40,44,52,76...|(100,[40,44,52,76...|         0|\n",
            "|    popular features| [popular, features]|(100,[48,55],[1.0...|(100,[48,55],[2.1...|         3|\n",
            "|          free tools|       [free, tools]|(100,[1,73],[1.0,...|(100,[1,73],[2.19...|         0|\n",
            "|             company|           [company]|    (100,[42],[1.0])|(100,[42],[1.2809...|         0|\n",
            "|           customers|         [customers]|    (100,[78],[1.0])|(100,[78],[2.1972...|         0|\n",
            "|            partners|          [partners]|    (100,[65],[1.0])|(100,[65],[2.1972...|         0|\n",
            "+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "hashing_tf = HashingTF(inputCol=\"tokens\", outputCol=\"raw_features\", numFeatures=100)\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "\n",
        "kmeans = KMeans(featuresCol=\"features\", k=5)\n",
        "\n",
        "pipeline = Pipeline(stages=[hashing_tf, idf, kmeans])\n",
        "model = pipeline.fit(cleaned_data)\n",
        "\n",
        "clusters = model.transform(cleaned_data)\n",
        "clusters.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Linear Regression*"
      ],
      "metadata": {
        "id": "ggy0z10MjVLT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPWjbpE1JyaY",
        "outputId": "3e8c0ca6-2532-472f-a551-ea1a203d046d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------+------------------+\n",
            "|             content|target|        prediction|\n",
            "+--------------------+------+------------------+\n",
            "| steps to create ...|    58| 58.00000002659291|\n",
            "|   featured articles|    17| 17.00000008912618|\n",
            "|     latest articles|    15| 14.99999984915986|\n",
            "|artificial intell...|    23|22.999999976121398|\n",
            "| instagram marketing|    19|18.999999996422623|\n",
            "|from hubspots vid...|    27| 26.99999997435789|\n",
            "|  marketing strategy|    18| 17.99999999374606|\n",
            "|from the hubspot ...|    32| 32.00000000090929|\n",
            "|        more content|    12|12.000000019642563|\n",
            "|visit the hubspot...|    23|22.999999973056724|\n",
            "|subscribe to hubs...|    33| 33.00000001514246|\n",
            "|join  fellow mark...|    44|43.999999987539894|\n",
            "|    popular features|    16|16.000000003816684|\n",
            "|          free tools|    10| 9.999999981397448|\n",
            "|             company|     7| 7.000000063765113|\n",
            "|           customers|     9| 9.000000025939695|\n",
            "|            partners|     8| 8.000000023263132|\n",
            "+--------------------+------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import length, col\n",
        "from pyspark.ml.feature import HashingTF, IDF, VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "hashing_tf = HashingTF(inputCol=\"tokens\", outputCol=\"raw_features\", numFeatures=100)\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "\n",
        "cleaned_data = cleaned_data.withColumn(\"target\", (length(col(\"content\")) % 100))  # Example target\n",
        "\n",
        "assembler = VectorAssembler(inputCols=[\"features\"], outputCol=\"final_features\")\n",
        "\n",
        "lr = LinearRegression(featuresCol=\"final_features\", labelCol=\"target\")\n",
        "\n",
        "pipeline = Pipeline(stages=[hashing_tf, idf, assembler, lr])\n",
        "model = pipeline.fit(cleaned_data)\n",
        "predictions = model.transform(cleaned_data)\n",
        "predictions.select(\"content\", \"target\", \"prediction\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Linear Regression*"
      ],
      "metadata": {
        "id": "ZLmXZN1rjfpt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrhjWUuGKdJZ",
        "outputId": "1062318b-a6b5-42c4-c8cf-1a51d7b1223e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+----------+\n",
            "|             content|label|prediction|\n",
            "+--------------------+-----+----------+\n",
            "| steps to create ...|  0.0|       0.0|\n",
            "|   featured articles|  1.0|       1.0|\n",
            "|     latest articles|  1.0|       1.0|\n",
            "|artificial intell...|  1.0|       1.0|\n",
            "| instagram marketing|  1.0|       1.0|\n",
            "|from hubspots vid...|  1.0|       1.0|\n",
            "|  marketing strategy|  0.0|       0.0|\n",
            "|from the hubspot ...|  0.0|       0.0|\n",
            "|        more content|  0.0|       0.0|\n",
            "|visit the hubspot...|  1.0|       1.0|\n",
            "|subscribe to hubs...|  1.0|       1.0|\n",
            "|join  fellow mark...|  0.0|       0.0|\n",
            "|    popular features|  0.0|       0.0|\n",
            "|          free tools|  0.0|       0.0|\n",
            "|             company|  1.0|       1.0|\n",
            "|           customers|  1.0|       1.0|\n",
            "|            partners|  0.0|       0.0|\n",
            "+--------------------+-----+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import length, col\n",
        "from pyspark.ml.feature import HashingTF, IDF\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "cleaned_data = cleaned_data.withColumn(\"label\", (length(col(\"content\")) % 2).cast(\"double\"))\n",
        "\n",
        "hashing_tf = HashingTF(inputCol=\"tokens\", outputCol=\"raw_features\", numFeatures=100)\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "\n",
        "# Step 3: Train SVM model\n",
        "svm = LinearSVC(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
        "\n",
        "# Step 4: Build pipeline\n",
        "pipeline = Pipeline(stages=[hashing_tf, idf, svm])\n",
        "svm_model = pipeline.fit(cleaned_data)\n",
        "predictions = svm_model.transform(cleaned_data)\n",
        "predictions.select(\"content\", \"label\", \"prediction\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Classification Model*"
      ],
      "metadata": {
        "id": "WZBHqOLRki0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_classification(cleaned_data):\n",
        "    hashing_tf = HashingTF(inputCol=\"tokens\", outputCol=\"raw_features\", numFeatures=100)\n",
        "    idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "\n",
        "    cleaned_data = cleaned_data.withColumn(\"label\", (length(col(\"content\")) % 2).cast(\"double\"))\n",
        "    train_data, test_data = cleaned_data.randomSplit([0.8, 0.2], seed=42)\n",
        "    lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
        "    pipeline = Pipeline(stages=[hashing_tf, idf, lr])\n",
        "    model = pipeline.fit(train_data)\n",
        "    predictions = model.transform(test_data).select(\"content\", \"label\", \"prediction\")\n",
        "\n",
        "    # Evaluate the model\n",
        "    evaluator = MulticlassClassificationEvaluator(\n",
        "        labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
        "    )\n",
        "    accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "    return predictions.toPandas(), f\"Accuracy: {accuracy}\""
      ],
      "metadata": {
        "id": "yhqFsZY8kWts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Reinforcement Learning*"
      ],
      "metadata": {
        "id": "WQv_e6gQkph5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csVYrtJcLgqJ",
        "outputId": "29b0992c-896d-4822-b62c-ab1de3e925f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "env = gym.make(\"CartPole-v1\", new_step_api=True)\n",
        "\n",
        "# Parameters\n",
        "action_space = env.action_space.n\n",
        "state_space = [20] * env.observation_space.shape[0]\n",
        "q_table = np.zeros(state_space + [action_space])\n",
        "alpha = 0.1  # Learning rate\n",
        "gamma = 0.9  # Discount factor\n",
        "epsilon = 0.1  # Exploration rate\n",
        "\n",
        "def discretize_state(state, bins):\n",
        "    state_bounds = list(zip(env.observation_space.low, env.observation_space.high))\n",
        "    state_bounds[1] = [-3.5, 3.5]\n",
        "    state_bounds[3] = [-3.5, 3.5]\n",
        "    ratios = [(state[i] - state_bounds[i][0]) / (state_bounds[i][1] - state_bounds[i][0]) for i in range(len(state))]\n",
        "    discrete_state = [int(r * (b - 1)) for r, b in zip(ratios, bins)]\n",
        "    return tuple(np.clip(discrete_state, 0, np.array(bins) - 1))\n",
        "\n",
        "for episode in range(1000):\n",
        "    state = discretize_state(env.reset(), state_space)\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = np.random.choice(action_space)\n",
        "        else:\n",
        "            action = np.argmax(q_table[state])\n",
        "        next_state_raw, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        next_state = discretize_state(next_state_raw, state_space)\n",
        "\n",
        "        q_table[state][action] += alpha * (\n",
        "            reward + gamma * np.max(q_table[next_state]) - q_table[state][action]\n",
        "        )\n",
        "        state = next_state\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLQzIUf6MsLp",
        "outputId": "4b741579-c624-49f8-a5c8-5fed664f74b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[[[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]]\n",
            "\n",
            "\n",
            "  [[[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]]\n",
            "\n",
            "\n",
            "  [[[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]]\n",
            "\n",
            "\n",
            "  ...\n",
            "\n",
            "\n",
            "  [[[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]]\n",
            "\n",
            "\n",
            "  [[[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]]\n",
            "\n",
            "\n",
            "  [[[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]]]\n",
            "\n",
            "\n",
            "\n",
            " [[[[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]]\n",
            "\n",
            "\n",
            "  [[[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]]\n",
            "\n",
            "\n",
            "  [[[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]]\n",
            "\n",
            "\n",
            "  ...\n",
            "\n",
            "\n",
            "  [[[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]]\n",
            "\n",
            "\n",
            "  [[[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]]\n",
            "\n",
            "\n",
            "  [[[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]]]\n",
            "\n",
            "\n",
            "\n",
            " [[[[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]]\n",
            "\n",
            "\n",
            "  [[[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]]\n",
            "\n",
            "\n",
            "  [[[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]]\n",
            "\n",
            "\n",
            "  ...\n",
            "\n",
            "\n",
            "  [[[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]]\n",
            "\n",
            "\n",
            "  [[[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]]\n",
            "\n",
            "\n",
            "  [[[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]]]\n",
            "\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            "\n",
            " [[[[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]]\n",
            "\n",
            "\n",
            "  [[[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]]\n",
            "\n",
            "\n",
            "  [[[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]]\n",
            "\n",
            "\n",
            "  ...\n",
            "\n",
            "\n",
            "  [[[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]]\n",
            "\n",
            "\n",
            "  [[[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]]\n",
            "\n",
            "\n",
            "  [[[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]]]\n",
            "\n",
            "\n",
            "\n",
            " [[[[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]]\n",
            "\n",
            "\n",
            "  [[[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]]\n",
            "\n",
            "\n",
            "  [[[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]]\n",
            "\n",
            "\n",
            "  ...\n",
            "\n",
            "\n",
            "  [[[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]]\n",
            "\n",
            "\n",
            "  [[[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]]\n",
            "\n",
            "\n",
            "  [[[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]]]\n",
            "\n",
            "\n",
            "\n",
            " [[[[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]]\n",
            "\n",
            "\n",
            "  [[[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]]\n",
            "\n",
            "\n",
            "  [[[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]]\n",
            "\n",
            "\n",
            "  ...\n",
            "\n",
            "\n",
            "  [[[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]]\n",
            "\n",
            "\n",
            "  [[[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]]\n",
            "\n",
            "\n",
            "  [[[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   ...\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]\n",
            "\n",
            "   [[0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    ...\n",
            "    [0. 0.]\n",
            "    [0. 0.]\n",
            "    [0. 0.]]]]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "print(q_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Calculating total reward*"
      ],
      "metadata": {
        "id": "BrwFUDlOk8zD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeZYkaoDMhhz",
        "outputId": "36dc2709-1f18-415d-edac-21dcf0c4c64c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete!\n",
            "Total reward: 13.0\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Create an environment (CartPole example)\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "state = env.reset()\n",
        "\n",
        "def discretize_state(state, bins, env):\n",
        "    state_bounds = list(zip(env.observation_space.low, env.observation_space.high))\n",
        "    state_bounds[1] = [-3.5, 3.5]  # Clip velocity to avoid issues\n",
        "    state_bounds[3] = [-3.5, 3.5]  # Clip angular velocity to avoid issues\n",
        "\n",
        "    # Normalize and map state to discrete bins\n",
        "    ratios = [(state[i] - state_bounds[i][0]) / (state_bounds[i][1] - state_bounds[i][0]) for i in range(len(state))]\n",
        "    discrete_state = [int(r * (bins - 1)) for r in ratios]\n",
        "    return tuple(np.clip(discrete_state, 0, bins - 1))  # Ensure indices are within bounds\n",
        "\n",
        "bins = 20  # Number of bins per state dimension\n",
        "state_space = [bins] * env.observation_space.shape[0]  # Discretized state space\n",
        "action_space = env.action_space.n  # Number of actions\n",
        "q_table = np.zeros(state_space + [action_space])  # State dimensions + actions\n",
        "\n",
        "# Training parameters\n",
        "alpha = 0.1  # Learning rate\n",
        "gamma = 0.9  # Discount factor\n",
        "epsilon = 0.1  # Exploration rate\n",
        "\n",
        "# Training loop\n",
        "for episode in range(1000):\n",
        "    state = discretize_state(env.reset(), bins, env)  # Discretize the initial state\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Epsilon-greedy action selection\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = np.random.choice(action_space)  # Exploration\n",
        "        else:\n",
        "            action = np.argmax(q_table[state])  # Exploitation\n",
        "\n",
        "        # Perform the action\n",
        "        next_state_raw, reward, done, info = env.step(action)  # Updated unpacking\n",
        "        next_state = discretize_state(next_state_raw, bins, env)\n",
        "\n",
        "        # Update Q-value\n",
        "        q_table[state][action] += alpha * (\n",
        "            reward + gamma * np.max(q_table[next_state]) - q_table[state][action]\n",
        "        )\n",
        "        state = next_state  # Move to the next state\n",
        "\n",
        "print(\"Training complete!\")\n",
        "\n",
        "# Evaluation loop (testing the trained Q-table)\n",
        "state = discretize_state(env.reset(), bins, env)  # Discretize the initial state\n",
        "done = False\n",
        "total_reward = 0\n",
        "\n",
        "while not done:\n",
        "    action = np.argmax(q_table[state])  # Choose the best action\n",
        "    next_state_raw, reward, done, info = env.step(action)  # Updated unpacking\n",
        "    next_state = discretize_state(next_state_raw, bins, env)\n",
        "    total_reward += reward\n",
        "    state = next_state\n",
        "    env.render()\n",
        "\n",
        "env.close()\n",
        "print(f\"Total reward: {total_reward}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*DBSCAN Algorithm*"
      ],
      "metadata": {
        "id": "X33QoHsxmMFP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "invYMNWPMjxA",
        "outputId": "3d57bd62-b4d1-408b-834a-6c532b73ede5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text:  steps to create an outstanding marketing plan fre... -> Cluster: -1\n",
            "Text: featured articles... -> Cluster: -1\n",
            "Text: latest articles... -> Cluster: -1\n",
            "Text: artificial intelligence... -> Cluster: -1\n",
            "Text: instagram marketing... -> Cluster: -1\n",
            "Text: from hubspots video library... -> Cluster: -1\n",
            "Text: marketing strategy... -> Cluster: -1\n",
            "Text: from the hubspot podcast network... -> Cluster: -1\n",
            "Text: more content... -> Cluster: -1\n",
            "Text: visit the hubspot blogs... -> Cluster: -1\n",
            "Text: subscribe to hubspots newsletters... -> Cluster: -1\n",
            "Text: join  fellow marketersthanks for subscribing... -> Cluster: -1\n",
            "Text: popular features... -> Cluster: -1\n",
            "Text: free tools... -> Cluster: -1\n",
            "Text: company... -> Cluster: -1\n",
            "Text: customers... -> Cluster: -1\n",
            "Text: partners... -> Cluster: -1\n"
          ]
        }
      ],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Step 1: Convert text into TF-IDF features\n",
        "texts = cleaned_data.select(\"content\").rdd.flatMap(lambda x: x).collect()\n",
        "vectorizer = TfidfVectorizer(max_features=100)\n",
        "X = vectorizer.fit_transform(texts).toarray()\n",
        "\n",
        "# Step 2: Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "labels = dbscan.fit_predict(X)\n",
        "\n",
        "# Show results\n",
        "for text, label in zip(texts, labels):\n",
        "    print(f\"Text: {text[:50]}... -> Cluster: {label}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Deep Learning*"
      ],
      "metadata": {
        "id": "6tiNE_ammQvb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drlLHWM3Mlnc",
        "outputId": "26bd931f-3220-4c27-dbfc-5d32e8b78577"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/util.py:55: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs; see https://jax.readthedocs.io/en/latest/aot.html. For example, replace xla_computation(f)(*xs) with jit(f).lower(*xs).compiler_ir('hlo'). See CHANGELOG.md for 0.4.30 for more examples.\n",
            "  from jax import xla_computation as _xla_computation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.4706 - loss: 0.6938\n",
            "Epoch 2/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.4706 - loss: 0.6931\n",
            "Epoch 3/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5294 - loss: 0.6928\n",
            "Epoch 4/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5294 - loss: 0.6925\n",
            "Epoch 5/5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5294 - loss: 0.6922\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "Text:  steps to create an outstanding marketing plan fre... -> Prediction: [0.49021408]\n",
            "Text: featured articles... -> Prediction: [0.48913017]\n",
            "Text: latest articles... -> Prediction: [0.48924866]\n",
            "Text: artificial intelligence... -> Prediction: [0.48957518]\n",
            "Text: instagram marketing... -> Prediction: [0.4894569]\n",
            "Text: from hubspots video library... -> Prediction: [0.48938987]\n",
            "Text: marketing strategy... -> Prediction: [0.48940018]\n",
            "Text: from the hubspot podcast network... -> Prediction: [0.4897352]\n",
            "Text: more content... -> Prediction: [0.48946774]\n",
            "Text: visit the hubspot blogs... -> Prediction: [0.48956522]\n",
            "Text: subscribe to hubspots newsletters... -> Prediction: [0.48944202]\n",
            "Text: join  fellow marketersthanks for subscribing... -> Prediction: [0.4898008]\n",
            "Text: popular features... -> Prediction: [0.48946884]\n",
            "Text: free tools... -> Prediction: [0.48958808]\n",
            "Text: company... -> Prediction: [0.48917255]\n",
            "Text: customers... -> Prediction: [0.4892852]\n",
            "Text: partners... -> Prediction: [0.48929685]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Step 1: Prepare data\n",
        "texts = cleaned_data.select(\"content\").rdd.flatMap(lambda x: x).collect()\n",
        "labels = [1 if len(text) % 2 == 0 else 0 for text in texts]  # Mock binary labels\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "X = tokenizer.texts_to_sequences(texts)\n",
        "X = pad_sequences(X, maxlen=100)\n",
        "\n",
        "import numpy as np\n",
        "# Step 2: Build model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=5000, output_dim=32, input_length=100),\n",
        "    GlobalAveragePooling1D(),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "labels = np.array(labels)\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 3: Train model\n",
        "model.fit(X, labels, epochs=5, batch_size=32)\n",
        "\n",
        "# Step 4: Make predictions\n",
        "predictions = model.predict(X)\n",
        "for text, pred in zip(texts, predictions):\n",
        "    print(f\"Text: {text[:50]}... -> Prediction: {pred}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Integrating the Code with Frontend#\n",
        "#Frontend is made using Gradio#"
      ],
      "metadata": {
        "id": "LMabrmLHmZV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lower, regexp_replace, split, explode, length\n",
        "from pyspark.ml.feature import HashingTF, IDF, VectorAssembler\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml import Pipeline\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "# Initialize Spark\n",
        "spark = SparkSession.builder.appName(\"ColabPySpark\").getOrCreate()\n",
        "\n",
        "# Data ingestion function\n",
        "def ingest_data():\n",
        "    url = \"https://blog.coupler.io/marketing-data-analytics/\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "    data = {\"content\": [item.get_text(strip=True) for item in soup.find_all(\"h2\")]}\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Data preprocessing function\n",
        "def preprocess_data():\n",
        "    df = ingest_data()\n",
        "    spark_df = spark.createDataFrame(df)\n",
        "    cleaned_data = (\n",
        "        spark_df.withColumn(\"content\", lower(col(\"content\")))\n",
        "                .withColumn(\"content\", regexp_replace(col(\"content\"), \"[^a-zA-Z\\\\s]\", \"\"))\n",
        "                .withColumn(\"tokens\", split(col(\"content\"), \"\\\\s+\"))\n",
        "    )\n",
        "    return cleaned_data\n",
        "from pyspark.sql.functions import size\n",
        "\n",
        "def compute_statistics(cleaned_data):\n",
        "    word_freq = (\n",
        "        cleaned_data.withColumn(\"word\", explode(col(\"tokens\")))\n",
        "                    .groupBy(\"word\")\n",
        "                    .count()\n",
        "    )\n",
        "    variance = word_freq.selectExpr(\"VAR_SAMP(count) as variance\").first()[\"variance\"]\n",
        "\n",
        "    # Add content_length and word_count columns\n",
        "    cleaned_data = cleaned_data.withColumn(\"content_length\", length(col(\"content\")))\n",
        "    cleaned_data = cleaned_data.withColumn(\"word_count\", size(col(\"tokens\")))  # Use size() for array length\n",
        "\n",
        "    # Compute correlation\n",
        "    correlation = cleaned_data.stat.corr(\"content_length\", \"word_count\")\n",
        "    return f\"Variance: {variance}\", f\"Correlation: {correlation}\"\n",
        "\n",
        "# K-Means Clustering\n",
        "def run_kmeans(cleaned_data):\n",
        "    hashing_tf = HashingTF(inputCol=\"tokens\", outputCol=\"raw_features\", numFeatures=100)\n",
        "    idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "    kmeans = KMeans(featuresCol=\"features\", k=5)\n",
        "    pipeline = Pipeline(stages=[hashing_tf, idf, kmeans])\n",
        "    model = pipeline.fit(cleaned_data)\n",
        "    clusters = model.transform(cleaned_data).select(\"content\", \"prediction\")\n",
        "    return clusters.toPandas()\n",
        "\n",
        "# Linear Regression\n",
        "def run_linear_regression(cleaned_data):\n",
        "    hashing_tf = HashingTF(inputCol=\"tokens\", outputCol=\"raw_features\", numFeatures=100)\n",
        "    idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "    cleaned_data = cleaned_data.withColumn(\"target\", (length(col(\"content\")) % 100))\n",
        "    assembler = VectorAssembler(inputCols=[\"features\"], outputCol=\"final_features\")\n",
        "    lr = LinearRegression(featuresCol=\"final_features\", labelCol=\"target\")\n",
        "    pipeline = Pipeline(stages=[hashing_tf, idf, assembler, lr])\n",
        "    model = pipeline.fit(cleaned_data)\n",
        "    predictions = model.transform(cleaned_data).select(\"content\", \"target\", \"prediction\")\n",
        "    return predictions.toPandas()\n",
        "\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "def run_classification(cleaned_data):\n",
        "    # Feature extraction (TF-IDF)\n",
        "    hashing_tf = HashingTF(inputCol=\"tokens\", outputCol=\"raw_features\", numFeatures=100)\n",
        "    idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "\n",
        "    # Add mock labels for classification (e.g., 1 for even-length content, 0 for odd-length content)\n",
        "    cleaned_data = cleaned_data.withColumn(\"label\", (length(col(\"content\")) % 2).cast(\"double\"))\n",
        "\n",
        "    # Train-test split\n",
        "    train_data, test_data = cleaned_data.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "    # Logistic Regression model\n",
        "    lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
        "\n",
        "    # Build pipeline\n",
        "    pipeline = Pipeline(stages=[hashing_tf, idf, lr])\n",
        "\n",
        "    # Train model\n",
        "    model = pipeline.fit(train_data)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.transform(test_data).select(\"content\", \"label\", \"prediction\")\n",
        "\n",
        "    # Evaluate the model\n",
        "    evaluator = MulticlassClassificationEvaluator(\n",
        "        labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
        "    )\n",
        "    accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "    return predictions.toPandas(), f\"Accuracy: {accuracy}\"\n",
        "\n",
        "# SVM Classification\n",
        "def run_svm(cleaned_data):\n",
        "    hashing_tf = HashingTF(inputCol=\"tokens\", outputCol=\"raw_features\", numFeatures=100)\n",
        "    idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "    cleaned_data = cleaned_data.withColumn(\"label\", (length(col(\"content\")) % 2).cast(\"double\"))\n",
        "    svm = LinearSVC(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
        "    pipeline = Pipeline(stages=[hashing_tf, idf, svm])\n",
        "    model = pipeline.fit(cleaned_data)\n",
        "    predictions = model.transform(cleaned_data).select(\"content\", \"label\", \"prediction\")\n",
        "    return predictions.toPandas()\n",
        "\n",
        "# DBSCAN Clustering\n",
        "def run_dbscan(cleaned_data):\n",
        "    texts = cleaned_data.select(\"content\").rdd.flatMap(lambda x: x).collect()\n",
        "    vectorizer = TfidfVectorizer(max_features=100)\n",
        "    X = vectorizer.fit_transform(texts).toarray()\n",
        "    dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "    labels = dbscan.fit_predict(X)\n",
        "    return pd.DataFrame({\"Text\": texts, \"Cluster\": labels})\n",
        "\n",
        "def run_deep_learning(cleaned_data):\n",
        "    texts = cleaned_data.select(\"content\").rdd.flatMap(lambda x: x).collect()\n",
        "    if not texts:  # Check if the dataset is empty\n",
        "        return pd.DataFrame({\"Error\": [\"No data available for deep learning.\"]})\n",
        "\n",
        "    labels = [1 if len(text) % 2 == 0 else 0 for text in texts]\n",
        "    tokenizer = Tokenizer(num_words=5000)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    X = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=100)\n",
        "\n",
        "    # Ensure the input data is float32\n",
        "    X = np.array(X, dtype=\"float32\")\n",
        "    labels = np.array(labels, dtype=\"float32\")\n",
        "\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=5000, output_dim=32, input_length=100),\n",
        "        GlobalAveragePooling1D(),\n",
        "        Dense(16, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    try:\n",
        "        model.fit(X, labels, epochs=5, batch_size=32, verbose=0)\n",
        "        predictions = model.predict(X)\n",
        "        return pd.DataFrame({\"Text\": texts, \"Prediction\": predictions.flatten()})\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame({\"Error\": [str(e)]})\n",
        "\n",
        "\n",
        "def run_reinforcement_learning():\n",
        "    env = gym.make(\"CartPole-v1\", new_step_api=True)\n",
        "    action_space = env.action_space.n\n",
        "    state_space = [20] * env.observation_space.shape[0]\n",
        "    q_table = np.zeros(state_space + [action_space])\n",
        "    alpha, gamma, epsilon = 0.1, 0.9, 0.1  # Learning rate, discount factor, exploration rate\n",
        "\n",
        "    def discretize_state(state, bins):\n",
        "        state_bounds = list(zip(env.observation_space.low, env.observation_space.high))\n",
        "        state_bounds[1] = [-3.5, 3.5]  # Clip velocity\n",
        "        state_bounds[3] = [-3.5, 3.5]  # Clip angular velocity\n",
        "        ratios = [(state[i] - state_bounds[i][0]) / (state_bounds[i][1] - state_bounds[i][0]) for i in range(len(state))]\n",
        "        discrete_state = [int(r * (b - 1)) for r, b in zip(ratios, bins)]\n",
        "        return tuple(np.clip(discrete_state, 0, np.array(bins) - 1))\n",
        "\n",
        "    for episode in range(500):  # Increased episodes for better training\n",
        "        state = discretize_state(env.reset(), state_space)\n",
        "        done = False\n",
        "        while not done:\n",
        "            # Epsilon-greedy action selection\n",
        "            if np.random.rand() < epsilon:\n",
        "                action = np.random.choice(action_space)  # Explore\n",
        "            else:\n",
        "                action = np.argmax(q_table[state])  # Exploit\n",
        "\n",
        "            # Perform action\n",
        "            next_state_raw, reward, terminated, truncated, _ = env.step(action)\n",
        "            next_state = discretize_state(next_state_raw, state_space)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # Reward scaling\n",
        "            reward = reward if not terminated else -100  # Penalize for termination\n",
        "\n",
        "            # Update Q-value\n",
        "            best_next_action = np.max(q_table[next_state])\n",
        "            q_table[state][action] += alpha * (reward + gamma * best_next_action - q_table[state][action])\n",
        "\n",
        "            # Move to the next state\n",
        "            state = next_state\n",
        "\n",
        "    if np.any(q_table):\n",
        "        return q_table.tolist()\n",
        "    else:\n",
        "        return \"Q-Table generation failed. Check reward structure or state discretization.\"\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from io import BytesIO\n",
        "import base64\n",
        "\n",
        "# Visualization: Word Frequency Distribution\n",
        "def plot_word_frequencies(cleaned_data):\n",
        "    word_freq = (\n",
        "        cleaned_data.withColumn(\"word\", explode(col(\"tokens\")))\n",
        "                    .groupBy(\"word\")\n",
        "                    .count()\n",
        "                    .orderBy(\"count\", ascending=False)\n",
        "    ).toPandas()\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x=\"count\", y=\"word\", data=word_freq.head(10), palette=\"viridis\")\n",
        "    plt.title(\"Top 10 Word Frequencies\")\n",
        "    plt.xlabel(\"Frequency\")\n",
        "    plt.ylabel(\"Words\")\n",
        "    return plt.gcf()\n",
        "\n",
        "# Visualization: K-Means Clusters\n",
        "def plot_kmeans_clusters(kmeans_result):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.countplot(x=\"prediction\", data=kmeans_result, palette=\"viridis\")\n",
        "    plt.title(\"K-Means Cluster Distribution\")\n",
        "    plt.xlabel(\"Cluster\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    return plt.gcf()\n",
        "\n",
        "# Visualization: DBSCAN Clusters\n",
        "def plot_dbscan_clusters(dbscan_result):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.countplot(x=\"Cluster\", data=dbscan_result, palette=\"viridis\")\n",
        "    plt.title(\"DBSCAN Cluster Distribution\")\n",
        "    plt.xlabel(\"Cluster\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    return plt.gcf()\n",
        "\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def topic_modeling(cleaned_data):\n",
        "    texts = cleaned_data.select(\"content\").rdd.flatMap(lambda x: x).collect()\n",
        "    vectorizer = CountVectorizer(max_features=1000, stop_words=\"english\")\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "    lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
        "    lda.fit(X)\n",
        "    topics = lda.components_\n",
        "    words = vectorizer.get_feature_names_out()\n",
        "    topic_words = [[words[i] for i in topic.argsort()[-10:]] for topic in topics]\n",
        "    return pd.DataFrame({\"Topic\": range(1, 6), \"Top Words\": topic_words})\n",
        "\n",
        "\n",
        "# Visualization: Reinforcement Learning Q-Table Heatmap\n",
        "def plot_q_table(q_table):\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.heatmap(q_table, annot=False, cmap=\"viridis\")\n",
        "    plt.title(\"Reinforcement Learning Q-Table Heatmap\")\n",
        "    plt.xlabel(\"Actions\")\n",
        "    plt.ylabel(\"States\")\n",
        "    return plt.gcf()\n",
        "# Gradio App\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# BIG DATA ANALYTICS PROJECT\")\n",
        "\n",
        "    # Section 1: Data Ingestion\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            ingest_btn = gr.Button(\"Ingest Data\")\n",
        "            ingest_output = gr.Dataframe(label=\"Ingested Data\")\n",
        "            ingest_btn.click(lambda: ingest_data(), inputs=[], outputs=[ingest_output])\n",
        "\n",
        "    # Section 2: Data Preprocessing\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            preprocess_btn = gr.Button(\"Preprocess Data\")\n",
        "            preprocess_output = gr.Dataframe(label=\"Preprocessed Data\")\n",
        "            preprocess_btn.click(lambda: preprocess_data().toPandas(), inputs=[], outputs=[preprocess_output])\n",
        "\n",
        "    # Section 3: Statistical Analysis\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            stats_btn = gr.Button(\"Statistical Analysis\")\n",
        "            stats_output = gr.Textbox(label=\"Statistical Analysis\")\n",
        "            stats_btn.click(lambda: compute_statistics(preprocess_data()), inputs=[], outputs=[stats_output])\n",
        "\n",
        "    # Section 4: Machine Learning Models\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            kmeans_btn = gr.Button(\"Run K-Means Clustering\")\n",
        "            kmeans_output = gr.Dataframe(label=\"K-Means Clusters\")\n",
        "            kmeans_btn.click(lambda: run_kmeans(preprocess_data()), inputs=[], outputs=[kmeans_output])\n",
        "\n",
        "        with gr.Column():\n",
        "            regression_btn = gr.Button(\"Run Linear Regression\")\n",
        "            regression_output = gr.Dataframe(label=\"Linear Regression Results\")\n",
        "            regression_btn.click(lambda: run_linear_regression(preprocess_data()), inputs=[], outputs=[regression_output])\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            svm_btn = gr.Button(\"Run SVM Classification\")\n",
        "            svm_output = gr.Dataframe(label=\"SVM Predictions\")\n",
        "            svm_btn.click(lambda: run_svm(preprocess_data()), inputs=[], outputs=[svm_output])\n",
        "\n",
        "        with gr.Column():\n",
        "            dbscan_btn = gr.Button(\"Run DBSCAN Clustering\")\n",
        "            dbscan_output = gr.Dataframe(label=\"DBSCAN Clusters\")\n",
        "            dbscan_btn.click(lambda: run_dbscan(preprocess_data()), inputs=[], outputs=[dbscan_output])\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            classification_btn = gr.Button(\"Run Classification Model\")\n",
        "            classification_output = gr.Dataframe(label=\"Classification Predictions\")\n",
        "            classification_accuracy = gr.Textbox(label=\"Classification Accuracy\")\n",
        "            classification_btn.click(\n",
        "                lambda: run_classification(preprocess_data()),\n",
        "                inputs=[],\n",
        "                outputs=[classification_output, classification_accuracy]\n",
        "            )\n",
        "\n",
        "        with gr.Column():\n",
        "            topic_btn = gr.Button(\"Run Topic Modeling\")\n",
        "            topic_output = gr.Dataframe(label=\"Topic Modeling\")\n",
        "            topic_btn.click(lambda: topic_modeling(preprocess_data()), inputs=[], outputs=[topic_output])\n",
        "\n",
        "\n",
        "\n",
        "    # Section 5: Deep Learning and Reinforcement Learning\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            deeplearning_btn = gr.Button(\"Run Deep Learning\")\n",
        "            deeplearning_output = gr.Dataframe(label=\"Deep Learning Predictions\")\n",
        "            deeplearning_btn.click(lambda: run_deep_learning(preprocess_data()), inputs=[], outputs=[deeplearning_output])\n",
        "\n",
        "        with gr.Column():\n",
        "            rl_btn = gr.Button(\"Run Reinforcement Learning\")\n",
        "            rl_output = gr.Textbox(label=\"Reinforcement Learning Q-Table\")\n",
        "            rl_btn.click(run_reinforcement_learning, inputs=[], outputs=[rl_output])\n",
        "\n",
        "    # Section 6: Visualizations\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            word_freq_btn = gr.Button(\"Visualize Word Frequencies\")\n",
        "            word_freq_plot = gr.Plot(label=\"Word Frequencies\")\n",
        "            word_freq_btn.click(lambda: plot_word_frequencies(preprocess_data()), inputs=[], outputs=[word_freq_plot])\n",
        "\n",
        "        with gr.Column():\n",
        "            kmeans_vis_btn = gr.Button(\"Visualize K-Means Clusters\")\n",
        "            kmeans_plot = gr.Plot(label=\"K-Means Cluster Distribution\")\n",
        "            kmeans_vis_btn.click(lambda: plot_kmeans_clusters(run_kmeans(preprocess_data())), inputs=[], outputs=[kmeans_plot])\n",
        "\n",
        "        with gr.Column():\n",
        "            dbscan_vis_btn = gr.Button(\"Visualize DBSCAN Clusters\")\n",
        "            dbscan_plot = gr.Plot(label=\"DBSCAN Cluster Distribution\")\n",
        "            dbscan_vis_btn.click(lambda: plot_dbscan_clusters(run_dbscan(preprocess_data())), inputs=[], outputs=[dbscan_plot])\n",
        "\n",
        "        with gr.Column():\n",
        "            q_table_vis_btn = gr.Button(\"Visualize Q-Table\")\n",
        "            q_table_plot = gr.Plot(label=\"Q-Table Heatmap\")\n",
        "            q_table_vis_btn.click(lambda: plot_q_table(run_reinforcement_learning()), inputs=[], outputs=[q_table_plot])\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "id": "EvK3p-HbRlQT",
        "outputId": "7db1e04f-b60b-4c00-f6ff-2567c4da9f83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://d565ed3c2bcf4db2da.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d565ed3c2bcf4db2da.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "dCaII2rBh7UI",
        "Gj5R6u07iKh9"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}